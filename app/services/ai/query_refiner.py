from __future__ import annotations

"""
QueryRefiner â€“ helper that removes noise (answer options, instructions, emojis, etc.)
from a raw user question so that the Vector Store receives a concise, highly-relevant
query for embedding search.

Example
-------
Input:
    Giáº£i thuáº­t nÃ o sau Ä‘Ã¢y xem xÃ©t Ä‘áº¿n Æ°á»›c lÆ°á»£ng tá»›i nÃºt Ä‘Ã­ch?
    Options:
    A. Depth-first search
    B. Best-first search
    C. A* search
    D. Greedy best-first search
Output:
    Thuáº­t toÃ¡n nÃ o xem xÃ©t Æ°á»›c lÆ°á»£ng (heuristic) tá»›i nÃºt Ä‘Ã­ch?

If the input is already short and clean, the output should be identical.
"""

from typing import Dict
from loguru import logger

from app.core.interfaces.ai_service import AIService, ChatMessage


class QueryRefiner:
    """Use LLM to produce a cleaner search query from a potentially noisy prompt."""

    SYSTEM_PROMPT: str = (
    "Báº¡n lÃ  má»™t trÃ¬nh lá»c truy váº¥n thÃ´ng minh. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  nháº­n má»™t cÃ¢u há»i thÃ´ tá»« ngÆ°á»i dÃ¹ng â€“ cÃ³ thá»ƒ chá»©a lá»±a chá»n Ä‘Ã¡p Ã¡n, vÃ­ dá»¥ minh há»a, hÆ°á»›ng dáº«n lÃ m bÃ i, Ä‘á»‹nh dáº¡ng tráº¯c nghiá»‡m, hoáº·c kÃ½ hiá»‡u khÃ´ng liÃªn quan â€“ "
    "vÃ  táº¡o ra má»™t cÃ¢u há»i ngáº¯n gá»n, rÃµ rÃ ng, khÃ¡i quÃ¡t vÃ  phÃ¹ há»£p Ä‘á»ƒ tÃ¬m kiáº¿m thÃ´ng tin cÃ³ liÃªn quan.\n\n"
    "Chá»‰ giá»¯ láº¡i pháº§n ná»™i dung trá»ng tÃ¢m â€“ Ä‘iá»u mÃ  ngÆ°á»i dÃ¹ng thá»±c sá»± muá»‘n há»i â€“ dÆ°á»›i dáº¡ng má»™t cÃ¢u Ä‘Æ¡n rÃµ rÃ ng. "
    "TrÃ¡nh giá»¯ láº¡i cÃ¡c lá»±a chá»n Ä‘Ã¡p Ã¡n (A., B., C., ...), tÃªn cá»¥ thá»ƒ nhÆ° G1, A, B, C náº¿u khÃ´ng cáº§n thiáº¿t, hoáº·c báº¥t ká»³ chi tiáº¿t mang tÃ­nh cÃ¡ biá»‡t hÃ³a mÃ  khÃ´ng giÃºp Ã­ch cho viá»‡c tÃ¬m tÃ i liá»‡u.\n\n"
    "VÃ­ dá»¥:\n"
    "Äáº§u vÃ o:\n"
    "Trong má»™t thÃ­ nghiá»‡m váº­t lÃ½, khi tÄƒng nhiá»‡t Ä‘á»™ thÃ¬ thá»ƒ tÃ­ch khÃ­ thay Ä‘á»•i nhÆ° tháº¿ nÃ o? Options: A. TÄƒng, B. Giáº£m, C. KhÃ´ng Ä‘á»•i\n"
    "Äáº§u ra:\n"
    "Quan há»‡ giá»¯a nhiá»‡t Ä‘á»™ vÃ  thá»ƒ tÃ­ch khÃ­ trong thÃ­ nghiá»‡m váº­t lÃ½\n\n"
    "Äáº§u vÃ o:\n"
    "Chá»n Ä‘Ã¡p Ã¡n Ä‘Ãºng: Ai lÃ  ngÆ°á»i viáº¿t tÃ¡c pháº©m Truyá»‡n Kiá»u? A. Nguyá»…n Du B. Nguyá»…n TrÃ£i C. Há»“ XuÃ¢n HÆ°Æ¡ng\n"
    "Äáº§u ra:\n"
    "TÃ¡c giáº£ cá»§a tÃ¡c pháº©m Truyá»‡n Kiá»u lÃ  ai?\n\n"
    "Äáº§u vÃ o:\n"
    "XÃ©t cÃ¢y tÃ¬m kiáº¿m sau, vá»›i táº­p Ä‘Ã­ch gá»“m 2 nÃºt G1 vÃ  G2. GiÃ¡ trá»‹ trÃªn má»—i cáº¡nh lÃ  chi phÃ­ di chuyá»ƒn giá»¯a 2 nÃºt ná»‘i 2 cáº¡nh Ä‘Ã³. HÃ£y cho biáº¿t thá»© tá»± duyá»‡t cÃ¡c nÃºt Ä‘áº¿n khi gáº·p nÃºt Ä‘Ã­ch (G1 hoáº·c G2) khi sá»­ dá»¥ng tÃ¬m kiáº¿m cá»±c tiá»ƒu. Options: A. ..., B. ...\n"
    "Äáº§u ra:\n"
    "Thá»© tá»± duyá»‡t cÃ¡c nÃºt trong tÃ¬m kiáº¿m cá»±c tiá»ƒu\n\n"
    "Náº¿u cÃ¢u há»i Ä‘áº§u vÃ o Ä‘Ã£ ngáº¯n gá»n, khÃ´ng chá»©a nhiá»…u, hÃ£y giá»¯ nguyÃªn. "
    "Chá»‰ tráº£ vá» má»™t cÃ¢u há»i Ä‘Æ¡n, rÃµ nghÄ©a, cÃ³ tÃ­nh khÃ¡i quÃ¡t, phÃ¹ há»£p Ä‘á»ƒ sá»­ dá»¥ng cho há»‡ thá»‘ng tÃ¬m kiáº¿m há»c sÃ¢u."
)



    def __init__(self, ai_service: AIService, cache_size: int = 256) -> None:  # noqa: D401
        """Create a new QueryRefiner.

        Args:
            ai_service: Instance of AIService used to perform the refinement.
            cache_size: Maximum number of refined queries to keep in memory.
        """
        self.ai_service = ai_service
        self._cache_size = cache_size
        self._cache: Dict[int, str] = {}

    async def refine(self, raw_question: str) -> str:  # noqa: D401
        """Return a concise query string extracted from *raw_question*.

        If the LLM call fails for any reason, the original question is returned to guarantee
        that the pipeline continues to work.
        """
        cache_key = hash(raw_question)
        if cache_key in self._cache:
            return self._cache[cache_key]

        messages = [
            ChatMessage(role="system", content=self.SYSTEM_PROMPT),
            ChatMessage(role="user", content=raw_question),
        ]

        try:
            response = await self.ai_service.chat(messages, temperature=0.0)
            if response.success and response.content.strip():
                refined = response.content.strip()
                if len(self._cache) < self._cache_size:
                    self._cache[cache_key] = refined
                logger.info(f"ðŸ” Query refined from '{raw_question}' to '{refined}'")
                return refined
        except Exception as exc:  
            logger.warning("QueryRefiner failed; falling back to raw question: %s", exc)

        # Fallback to original question
        return raw_question 