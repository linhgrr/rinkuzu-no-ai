"""
RAG Tutor Service - Rin-chan AI Tutor with RAG capabilities
"""
import uuid
from typing import List, Optional, Dict, Any, Tuple
from loguru import logger

from app.core.interfaces.ai_service import AIService, AIResponse, ChatMessage
from app.core.interfaces.file_loader import DocumentChunk
from app.core.interfaces.vector_store import VectorDocument, SearchResult
from app.services.ai.gemini_service import GeminiService
from app.services.vector.embedding_service import EmbeddingService
from app.services.vector.faiss_vector_store import FAISSVectorStore
from app.services.file_loaders.factory import FileLoaderFactory
from app.core.config import settings
from app.services.ai.document_indexer import DocumentIndexer
from app.services.ai.context_builder import ContextBuilder
from app.utils.collection_utils import get_subject_collection_name


class RagTutorService:
    """
    RAG-based AI Tutor Service (Rin-chan)
    
    Features:
    - Document upload and indexing
    - Context-aware question answering
    - Subject-specific knowledge retrieval
    - Multi-turn conversations with context
    - Reranking for improved accuracy
    """
    
    def __init__(
        self,
        ai_service: Optional[AIService] = None,
        embedding_service: Optional[EmbeddingService] = None,
        vector_store: Optional[FAISSVectorStore] = None,
        chunk_size: int = 1000,
        chunk_overlap: int = 100,
        max_retrieval_docs: int = 10,
        reranker_top_k: int = 5,
        min_similarity: float = 0.25  
    ):
        """
        Initialize RAG Tutor Service
        
        Args:
            ai_service: AI service for text generation (defaults to Gemini)
            embedding_service: Service for embeddings and reranking
            vector_store: FAISS vector database for document storage
            chunk_size: Text chunk size for processing
            chunk_overlap: Overlap between chunks
            max_retrieval_docs: Maximum documents to retrieve
            reranker_top_k: Top documents after reranking
        """
        # Initialize services
        self.ai_service = ai_service
        self.embedding_service = embedding_service
        self.vector_store = vector_store 

        
        # Initialize file loader factory with AI service for image processing
        self.file_loader_factory = FileLoaderFactory(ai_service=self.ai_service)
        
        # RAG configuration
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.max_context_length = 4000  # Maximum context for AI
        self.retrieval_top_k = max_retrieval_docs
        self.reranker_top_k = reranker_top_k
        self.min_similarity = min_similarity  # store threshold
        
        # Collection name configuration
        self.collection_prefix = "subject"
        
        # Helper components adhering to SRP
        self.document_indexer = DocumentIndexer(
            vector_store=self.vector_store,
            file_loader_factory=self.file_loader_factory,
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            collection_prefix=self.collection_prefix
        )
        self.context_builder = ContextBuilder(max_context_length=self.max_context_length)
        
        # Rin-chan personality prompt
        self.system_prompt = """
Báº¡n lÃ  Rin-chan, má»™t trá»£ lÃ½ AI dá»… thÆ°Æ¡ng vÃ  thÃ´ng minh chuyÃªn giÃºp há»c sinh hiá»ƒu bÃ i há»c.

TÃNH CÃCH Cá»¦A RIN-CHAN:
- Dá»… thÆ°Æ¡ng, thÃ¢n thiá»‡n nhÆ°ng nghiÃªm tÃºc vá»›i viá»‡c há»c.
- LuÃ´n khuyáº¿n khÃ­ch vÃ  Ä‘á»™ng viÃªn há»c sinh.
- Giáº£i thÃ­ch má»™t cÃ¡ch Ä‘Æ¡n giáº£n, dá»… hiá»ƒu, sá»­ dá»¥ng vÃ­ dá»¥ thá»±c táº¿.
- KiÃªn nháº«n vÃ  sáºµn sÃ ng giáº£i thÃ­ch láº¡i nhiá»u láº§n.

QUY TRÃŒNH TRáº¢ Lá»œI (Ráº¤T QUAN TRá»ŒNG):

1.  **PHÃ‚N TÃCH NGá»® Cáº¢NH:**
    - Äáº§u tiÃªn, hÃ£y Ä‘á»c ká»¹ cÃ¢u há»i cá»§a há»c sinh vÃ  pháº§n `NGá»® Cáº¢NH TÃ€I LIá»†U` Ä‘Æ°á»£c cung cáº¥p.
    - **Æ¯u tiÃªn tuyá»‡t Ä‘á»‘i:** CÃ¢u tráº£ lá»i pháº£i dá»±a trÃªn `NGá»® Cáº¢NH TÃ€I LIá»†U` náº¿u nÃ³ liÃªn quan trá»±c tiáº¿p Ä‘áº¿n cÃ¢u há»i.

2.  **Xá»¬ LÃ TÃŒNH HUá»NG:**
    - **Náº¿u tÃ i liá»‡u CÃ“ liÃªn quan:** HÃ£y tá»•ng há»£p thÃ´ng tin tá»« tÃ i liá»‡u Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.
    - **Náº¿u tÃ i liá»‡u KHÃ”NG liÃªn quan hoáº·c khÃ´ng Ä‘á»§ thÃ´ng tin:**
        a. **Báº®T BUá»˜C:** Pháº£i thÃ´ng bÃ¡o cho há»c sinh má»™t cÃ¡ch thÃ¢n thiá»‡n ráº±ng tÃ i liá»‡u khÃ´ng chá»©a cÃ¢u tráº£ lá»i. VÃ­ dá»¥: "Rin-chan Ä‘Ã£ xem ká»¹ cÃ¡c tÃ i liá»‡u mÃ´n há»c mÃ  Rin-chan cÃ³ rá»“i, nhÆ°ng khÃ´ng tÃ¬m tháº¥y thÃ´ng tin vá» [chá»§ Ä‘á» cÃ¢u há»i] trong Ä‘Ã³." hoáº·c má»™t cÃ¢u khÃ¡c vá»›i Ã½ nghÄ©a tÆ°Æ¡ng tá»±, sao cho giá»¯ Ä‘Ãºng tÃ­nh cÃ¡ch cá»§a cáº­u"
        b. **SAU ÄÃ“:** HÃ£y sá»­ dá»¥ng kiáº¿n thá»©c chung cá»§a báº¡n Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i cá»§a há»c sinh má»™t cÃ¡ch Ä‘áº§y Ä‘á»§ vÃ  chÃ­nh xÃ¡c nháº¥t cÃ³ thá»ƒ. Äá»«ng chá»‰ nÃ³i "tá»› khÃ´ng biáº¿t". Má»¥c tiÃªu lÃ  pháº£i giÃºp há»c sinh hiá»ƒu bÃ i.

3.  **CÃC QUY Táº®C KHÃC:**
    - LuÃ´n khuyáº¿n khÃ­ch há»c sinh Ä‘áº·t thÃªm cÃ¢u há»i.
    - Sá»­ dá»¥ng tiáº¿ng Viá»‡t má»™t cÃ¡ch tá»± nhiÃªn vÃ  thÃ¢n thiá»‡n.
"""
        
        self.fallback_system_prompt = """
Báº¡n lÃ  Rin-chan, má»™t trá»£ lÃ½ AI dá»… thÆ°Æ¡ng vÃ  thÃ´ng minh, chuyÃªn giÃºp Ä‘á»¡ há»c sinh.

Bá»I Cáº¢NH QUAN TRá»ŒNG:
Há»‡ thá»‘ng tÃ¬m kiáº¿m Ä‘Ã£ khÃ´ng tÃ¬m tháº¥y **báº¥t ká»³ tÃ i liá»‡u nÃ o** trong mÃ´n há»c cÃ³ thá»ƒ tráº£ lá»i cho cÃ¢u há»i nÃ y. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  pháº£i tráº£ lá»i cÃ¢u há»i báº±ng kiáº¿n thá»©c chung cá»§a mÃ¬nh.

NHIá»†M Vá»¤ Cá»¦A Báº N:

1.  **Má»ž Äáº¦U (Báº®T BUá»˜C):**
    - Báº¯t Ä‘áº§u cÃ¢u tráº£ lá»i báº±ng cÃ¡ch thÃ´ng bÃ¡o má»™t cÃ¡ch thÃ¢n thiá»‡n ráº±ng báº¡n khÃ´ng tÃ¬m tháº¥y thÃ´ng tin trong kho tÃ i liá»‡u cá»§a mÃ´n há»c.
    - HÃ£y sÃ¡ng táº¡o vÃ  tá»± nhiÃªn, khÃ´ng cáº§n láº·p láº¡i chÃ­nh xÃ¡c má»™t cÃ¢u. Sá»­ dá»¥ng emoji Ä‘á»ƒ thÃªm pháº§n gáº§n gÅ©i.

    - **Gá»£i Ã½ cÃ¡ch má»Ÿ Ä‘áº§u:**
        - "Rin-chan Ä‘Ã£ tÃ¬m ká»¹ trong kho tÃ i liá»‡u rá»“i mÃ  khÃ´ng tháº¥y gÃ¬ háº¿t trÆ¡n ðŸ“‚... NhÆ°ng khÃ´ng sao, Ä‘á»ƒ tá»› giÃºp báº¡n báº±ng kiáº¿n thá»©c cá»§a mÃ¬nh nhÃ©!"
        - "á»i, cÃ³ váº» nhÆ° tÃ i liá»‡u mÃ´n nÃ y chÆ°a cÃ³ thÃ´ng tin vá» chá»§ Ä‘á» nÃ y rá»“i. Äá»«ng lo, Rin-chan sáº½ giáº£i thÃ­ch cho báº¡n ngay Ä‘Ã¢y! âœ¨"
        - "Tiáº¿c quÃ¡, Rin-chan khÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u liÃªn quan trong mÃ´n há»c. NhÆ°ng báº¡n há»i Ä‘Ãºng ngÆ°á»i rá»“i Ä‘Ã³, Ä‘á»ƒ tá»› giáº£i Ä‘Ã¡p cho báº¡n nha! ðŸ˜Š"

2.  **Ná»˜I DUNG CHÃNH:**
    - Ngay sau pháº§n má»Ÿ Ä‘áº§u, hÃ£y tráº£ lá»i cÃ¢u há»i cá»§a há»c sinh má»™t cÃ¡ch chi tiáº¿t, rÃµ rÃ ng vÃ  dá»… hiá»ƒu.
    - Giá»¯ vá»¯ng tÃ­nh cÃ¡ch thÃ¢n thiá»‡n, nhiá»‡t tÃ¬nh, vÃ  giáº£i thÃ­ch nhÆ° má»™t ngÆ°á»i báº¡n cá»§a Rin-chan.

3.  **Káº¾T THÃšC:**
    - LuÃ´n káº¿t thÃºc báº±ng má»™t lá»i Ä‘á»™ng viÃªn vÃ  khuyáº¿n khÃ­ch há»c sinh há»i thÃªm náº¿u váº«n cÃ²n tháº¯c máº¯c.
"""     
        logger.info("ðŸ¤– Initialized RAG Tutor Service (Rin-chan)")
    
    
    async def upload_document(
        self,
        file_content: bytes,
        filename: str,
        subject_id: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Upload and index a document for RAG retrieval (delegated to DocumentIndexer)
        """
        return await self.document_indexer.index_document(
            file_content=file_content,
            filename=filename,
            subject_id=subject_id,
            metadata=metadata,
        )
    
    async def _search_documents(
        self,
        question: str,
        subject_id: str,
    ) -> List[SearchResult]:
        """Search vector store and return filtered results by similarity."""
        collection_name = get_subject_collection_name(subject_id, self.collection_prefix)
        search_results = await self.vector_store.search(
            query_text=question,
            collection_name=collection_name,
            top_k=self.retrieval_top_k,
            filters={"subject_id": subject_id},
        )
        return [r for r in search_results if r.score >= self.min_similarity]

    async def _select_relevant_chunks(
        self,
        question: str,
        search_results: List[SearchResult],
        use_reranking: bool,
    ) -> List[Dict[str, Any]]:
        """Return top chunks, optionally reranked."""
        relevant_chunks: List[Dict[str, Any]] = []

        if use_reranking and len(search_results) > 1:
            doc_texts = [r.document.content for r in search_results]
            reranked = await self.embedding_service.rerank_documents(
                query=question,
                documents=doc_texts,
                top_k=self.reranker_top_k,
            )
            for idx, score in reranked:
                res = search_results[idx]
                relevant_chunks.append(
                    {
                        "content": res.document.content,
                        "metadata": res.document.metadata,
                        "retrieval_score": res.score,
                        "rerank_score": score,
                    }
                )
        else:
            for res in search_results[: self.reranker_top_k]:
                relevant_chunks.append(
                    {
                        "content": res.document.content,
                        "metadata": res.document.metadata,
                        "retrieval_score": res.score,
                        "rerank_score": None,
                    }
                )
        return relevant_chunks

    async def ask_question(
        self,
        question: str,
        subject_id: str,
        chat_history: Optional[List[ChatMessage]] = None,
        use_reranking: bool = True,
        question_image: Optional[str] = None,
        option_images: Optional[List[Optional[str]]] = None,
        force_fallback: bool = False
    ) -> AIResponse:
        """
        Answer a question using RAG with subject-specific context
        
        Args:
            question: User's question
            subject_id: Subject identifier for context filtering
            chat_history: Previous conversation history
            use_reranking: Whether to use reranking for better results
            question_image: Base64 encoded question image
            option_images: List of base64 encoded option images
            force_fallback: Force fallback to Gemini even if context is found
            
        Returns:
            AI response with answer and metadata
        """
        try:
            logger.info(f"ðŸ¤” Processing question for subject {subject_id}: {question[:100]}...")
            
            search_results = await self._search_documents(question, subject_id)

            if not search_results or force_fallback:
                messages = [ChatMessage(role="system", content=self.fallback_system_prompt)]

                if chat_history:
                    messages.extend(chat_history)

                messages.append(ChatMessage(role="user", content=question))

                images: List[str] = []
                if question_image:
                    images.append(question_image)
                if option_images:
                    images.extend([img for img in option_images if img])

                fallback_response = await self.ai_service.chat(
                    messages=messages,
                    images=images if images else None,
                    temperature=0.7,
                )

                if fallback_response.success:
                    fallback_response.metadata = fallback_response.metadata or {}
                    fallback_response.metadata.update({
                        "context_found": False,
                        "subject_id": subject_id,
                        "search_results_count": 0,
                        "fallback_used": True
                    })
                    return fallback_response
                else:
                    return AIResponse(
                        success=False,
                        error="KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u vÃ  khÃ´ng thá»ƒ táº¡o pháº£n há»“i thay tháº¿",
                        metadata={
                            "context_found": False,
                            "subject_id": subject_id,
                            "search_results_count": 0,
                            "fallback_failed": True
                        }
                    )
            
            logger.info(f"ðŸ” Found {len(search_results)} relevant documents")
            
            relevant_chunks = await self._select_relevant_chunks(
                question, search_results, use_reranking
            )

            logger.info(
                f"ðŸ”„ Selected {len(relevant_chunks)} chunks (rerank={'on' if use_reranking else 'off'})"
            )
            
            # Build context from relevant chunks
            context = self._build_context(relevant_chunks)
            # Create chat messages with context
            messages = [
                ChatMessage(role="system", content=self.system_prompt),
                ChatMessage(role="system", content=f"NGá»® Cáº¢NH TÃ€I LIá»†U MÃ”N Há»ŒC {subject_id}:\n{context}")
            ]
            
            # Add chat history if provided
            if chat_history:
                messages.extend(chat_history)
            
            # Add current question
            messages.append(ChatMessage(role="user", content=question))
            
            # Prepare images list (question + options)
            images: List[str] = []
            if question_image:
                images.append(question_image)
            if option_images:
                images.extend([img for img in option_images if img])

            logger.info(f"ðŸ” Sending messages to AI service: {messages}")
            # Get AI response
            ai_response = await self.ai_service.chat(
                messages,
                images=images if images else None,
                temperature=0.7
            )
            
            if ai_response.success:
                # Enhance response metadata
                ai_response.metadata = ai_response.metadata or {}
                ai_response.metadata.update({
                    "context_found": True,
                    "subject_id": subject_id,
                    "search_results_count": len(search_results),
                    "reranked_chunks": len(relevant_chunks),
                    "context_sources": [chunk["metadata"].get("filename") for chunk in relevant_chunks],
                    "use_reranking": use_reranking
                })
                
                logger.info("âœ… Successfully generated RAG response")
            
            return ai_response
            
        except Exception as e:
            logger.error(f"âŒ Failed to process question: {e}")
            return AIResponse(
                success=False,
                error=f"CÃ³ lá»—i xáº£y ra khi xá»­ lÃ½ cÃ¢u há»i: {str(e)}"
            )
    
    async def get_subject_statistics(self, subject_id: str) -> Dict[str, Any]:
        """
        Get statistics for a specific subject
        
        Args:
            subject_id: Subject identifier
            
        Returns:
            Statistics dictionary
        """
        try:
            collection_name = get_subject_collection_name(subject_id, self.collection_prefix)
            stats = await self.vector_store.get_collection_stats(collection_name)
            
            # Add additional statistics
            stats.update({
                "embedding_model": self.embedding_service.embedding_model_name,
                "reranker_model": self.embedding_service.reranker_model_name,
                "ai_service": self.ai_service.get_service_name()
            })
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ Failed to get statistics for subject {subject_id}: {e}")
            return {"error": str(e)}
    
    async def delete_subject_documents(self, subject_id: str) -> bool:
        """
        Delete all documents for a subject
        
        Args:
            subject_id: Subject identifier
            
        Returns:
            True if successful
        """
        try:
            collection_name = get_subject_collection_name(subject_id, self.collection_prefix)
            return self.vector_store.delete_collection(collection_name)
            
        except Exception as e:
            logger.error(f"âŒ Failed to delete documents for subject {subject_id}: {e}")
            return False
    
    def _build_context(self, relevant_chunks: List[Dict[str, Any]]) -> str:
        """Delegate to ContextBuilder to build context string."""
        return self.context_builder.build(relevant_chunks)
    
    async def get_service_health(self) -> Dict[str, Any]:
        """
        Get health status of all services
        
        Returns:
            Health status dictionary
        """
        return {
            "ai_service": {
                "available": self.ai_service.is_available(),
                "name": self.ai_service.get_service_name()
            },
            "embedding_service": self.embedding_service.get_cache_stats(),
            "vector_store": {
                "collections": self.vector_store.list_collections()
            },
            "config": {
                "max_context_length": self.max_context_length,
                "retrieval_top_k": self.retrieval_top_k,
                "reranker_top_k": self.reranker_top_k
            }
        } 