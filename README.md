# ğŸ¤– AI Service - Educational Assistant Platform

A comprehensive FastAPI-based microservice for educational applications, featuring **Rin-chan**, an intelligent RAG-based tutor system with advanced document processing and contextual question answering.

## ğŸŒŸ Key Features

- **ğŸ“š RAG Tutoring**: Upload learning materials and ask intelligent questions
- **ğŸ” Smart Search**: Vector search with reranking for accurate answers  
- **ğŸ’¬ Multi-turn Chat**: Contextual conversations with AI tutor
- **ğŸ“„ Document Processing**: Support for PDF, DOCX, and other formats
- **ğŸŒ Multi-language**: Vietnamese and English support
- **âš¡ High Performance**: Optimized embedding and generation pipeline
- **ğŸ—ï¸ Clean Architecture**: OOP design with dependency injection
- **ğŸ”Œ Plug-and-play**: Easy to swap AI models and vector stores

## ğŸ› ï¸ Technology Stack

- **ğŸ§  AI Models**: Google Gemini 2.0 Flash (with automatic fallback)
- **ğŸ”¤ Embeddings**: GTE Multilingual Base & Reranker (Hit@1 > 84%)
- **ğŸ—„ï¸ Vector DB**: ChromaDB with persistent storage
- **ğŸš€ Framework**: FastAPI with async/await support
- **ğŸ“ File Processing**: 
  - **PDF**: PyMuPDF (advanced layout preservation)
  - **DOCX**: python-docx (tables, formatting)
  - **PPTX**: python-pptx (slides, notes)
  - **Images**: Gemini Vision (OCR, analysis)
- **ğŸ”’ Security**: Input validation, API key auth, CORS
- **ğŸ“Š Monitoring**: Comprehensive logging with loguru

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
cd ai_service

# Install dependencies
pip install -r requirements.txt
```

### 2. Environment Configuration

Create a `.env` file based on the example:

```bash
# Copy the example file
cp env.example .env

# Then edit .env with your settings
```

**Required Configuration:**
```bash
# Google Gemini API Keys (get from https://ai.google.dev/)
GEMINI_KEYS=your_gemini_key_1,your_gemini_key_2,your_gemini_key_3

# File upload settings
ALLOWED_FILE_TYPES=pdf,docx,pptx,png,jpg,jpeg,gif,bmp,webp
MAX_FILE_SIZE=52428800  # 50MB

# Vector database
VECTOR_DB_PATH=./vector_db
```

**Optional Settings:**
```bash
DEBUG=true
LOG_LEVEL=INFO
CHUNK_SIZE=1000
CHUNK_OVERLAP=100
MAX_RETRIEVAL_DOCS=10
RERANKER_TOP_K=3
```

### 3. Run the Service

**Option 1: Quick Start (Recommended)**
```bash
# Use the startup script with automatic checks
python start.py
```

**Option 2: Manual Start**
```bash
# Development mode with auto-reload
python main.py

# Or using uvicorn directly
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

The service will be available at:
- **API**: http://localhost:8000
- **Documentation**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## ğŸ“– API Usage Examples

### Upload Learning Material

**Upload PDF/DOCX/PPTX:**
```bash
curl -X POST "http://localhost:8000/v1/tutor/upload-material" \
  -F "file=@lecture_notes.pdf" \
  -F "subject_id=computer_science_101" \
  -F "metadata={\"course\": \"CS101\", \"chapter\": \"1\"}"
```

**Upload Images with Gemini Vision:**
```bash
curl -X POST "http://localhost:8000/v1/tutor/upload-material" \
  -F "file=@diagram.png" \
  -F "subject_id=computer_science_101" \
  -F "metadata={\"type\": \"diagram\", \"topic\": \"CPU architecture\"}"
```

**Upload PowerPoint Presentation:**
```bash
curl -X POST "http://localhost:8000/v1/tutor/upload-material" \
  -F "file=@slides.pptx" \
  -F "subject_id=computer_science_101" \
  -F "metadata={\"lecture\": \"Introduction to AI\"}"
```

### Ask Rin-chan a Question

```bash
curl -X POST "http://localhost:8000/v1/tutor/ask-question" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "CPU hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o?",
    "subject_id": "computer_science_101",
    "use_reranking": true
  }'
```

### Multi-turn Conversation

```bash
curl -X POST "http://localhost:8000/v1/tutor/ask-question" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Cho vÃ­ dá»¥ vá» cache miss?",
    "subject_id": "computer_science_101",
    "chat_history": [
      {"role": "user", "content": "CPU hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o?"},
      {"role": "assistant", "content": "CPU xá»­ lÃ½ dá»¯ liá»‡u thÃ´ng qua cÃ¡c chu ká»³ fetch, decode, execute..."}
    ]
  }'
```

### List Available Subjects

```bash
curl -X GET "http://localhost:8000/v1/tutor/subjects"
```

### General AI Chat

```bash
curl -X POST "http://localhost:8000/v1/ai/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "system", "content": "You are a helpful assistant"},
      {"role": "user", "content": "Explain quantum computing"}
    ],
    "temperature": 0.7
  }'
```

## ğŸ—ï¸ Architecture Overview

### Core Components

```
ai_service/
â”œâ”€â”€ main.py                 # FastAPI application entry point
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py      # Environment configuration
â”‚   â”‚   â””â”€â”€ interfaces/    # Abstract interfaces
â”‚   â”‚       â”œâ”€â”€ ai_service.py
â”‚   â”‚       â”œâ”€â”€ file_loader.py
â”‚   â”‚       â””â”€â”€ vector_store.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”‚   â”œâ”€â”€ gemini_service.py      # Gemini AI implementation
â”‚   â”‚   â”‚   â””â”€â”€ rag_tutor_service.py   # Main RAG service
â”‚   â”‚   â”œâ”€â”€ vector/
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding_service.py   # GTE embeddings
â”‚   â”‚   â”‚   â””â”€â”€ chroma_store.py        # ChromaDB integration
â”‚   â”‚   â””â”€â”€ file_loaders/
â”‚   â”‚       â”œâ”€â”€ pdf_loader.py          # PDF processing
â”‚   â”‚       â”œâ”€â”€ docx_loader.py         # DOCX processing
â”‚   â”‚       â””â”€â”€ factory.py             # Loader factory
â”‚   â”œâ”€â”€ api/v1/
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ tutor.py   # RAG tutoring endpoints
â”‚   â”‚   â”‚   â””â”€â”€ ai.py      # General AI endpoints
â”‚   â”‚   â””â”€â”€ __init__.py    # API router
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ requests.py    # Pydantic request models
â”‚   â”‚   â””â”€â”€ responses.py   # Pydantic response models
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ dependencies.py    # FastAPI dependencies
â”‚       â””â”€â”€ key_rotation.py    # API key management
```

### Design Patterns

1. **Interface Segregation**: Abstract interfaces for all major components
2. **Dependency Injection**: Services injected via FastAPI Depends
3. **Factory Pattern**: Automatic file loader selection
4. **Strategy Pattern**: Pluggable AI services and vector stores
5. **Observer Pattern**: Key rotation with health monitoring

## ğŸ”§ Configuration

### Environment Variables

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `GEMINI_KEYS` | âœ… | - | Comma-separated Gemini API keys |
| `DEBUG` | âŒ | false | Enable debug mode |
| `HOST` | âŒ | 0.0.0.0 | Server host |
| `PORT` | âŒ | 8000 | Server port |
| `LOG_LEVEL` | âŒ | INFO | Logging level |
| `VECTOR_DB_PATH` | âŒ | ./vector_db | ChromaDB storage path |
| `EMBEDDING_MODEL` | âŒ | Alibaba-NLP/gte-multilingual-base | HuggingFace model |
| `RERANKER_MODEL` | âŒ | Alibaba-NLP/gte-multilingual-reranker-base | Reranker model |
| `CHUNK_SIZE` | âŒ | 1000 | Text chunk size |
| `CHUNK_OVERLAP` | âŒ | 100 | Chunk overlap |
| `MAX_RETRIEVAL_DOCS` | âŒ | 10 | Max documents to retrieve |
| `RERANKER_TOP_K` | âŒ | 3 | Top documents after reranking |
| `MAX_FILE_SIZE` | âŒ | 52428800 | Max upload size (50MB) |
| `ALLOWED_FILE_TYPES` | âŒ | pdf,docx,txt | Supported file types |
| `API_KEY` | âŒ | - | Optional API authentication |
| `CORS_ORIGINS` | âŒ | * | CORS allowed origins |

### RAG Pipeline Configuration

The system uses a sophisticated RAG pipeline:

1. **Document Processing**: Files are chunked with configurable size and overlap
2. **Embedding**: GTE Multilingual Base creates semantic vectors
3. **Storage**: ChromaDB stores documents with metadata filtering
4. **Retrieval**: Vector similarity search finds relevant chunks
5. **Reranking**: GTE Reranker improves result quality (Hit@1 > 84%)
6. **Generation**: Gemini 2.0 Flash generates contextual answers

## ğŸš€ Production Deployment

### Docker Deployment

```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 8000
CMD ["python", "main.py"]
```

### Environment Variables for Production

```bash
DEBUG=false
LOG_LEVEL=INFO
API_KEY=your_production_api_key
CORS_ORIGINS=https://yourdomain.com,https://app.yourdomain.com
VECTOR_DB_PATH=/app/data/vector_db
```

## ğŸ”Œ Extending the System

### Adding New AI Services

1. Implement the `AIService` interface:

```python
from app.core.interfaces.ai_service import AIService, AIResponse

class CustomAIService(AIService):
    async def generate_text(self, prompt: str, **kwargs) -> AIResponse:
        # Your implementation
        pass
    
    async def chat(self, messages: List[ChatMessage], **kwargs) -> AIResponse:
        # Your implementation
        pass
```

2. Register in dependencies:

```python
# app/utils/dependencies.py
@lru_cache()
def get_custom_ai_service() -> CustomAIService:
    return CustomAIService()
```

### Adding New File Loaders

```python
from app.core.interfaces.file_loader import FileLoader, DocumentChunk

class CustomLoader(FileLoader):
    async def load(self, file_stream: BinaryIO, filename: str) -> List[DocumentChunk]:
        # Your implementation
        pass
    
    def supports_file_type(self, file_extension: str) -> bool:
        return file_extension in ['custom']

# Register in factory
from app.services.file_loaders.factory import file_loader_factory
file_loader_factory.register_loader('custom', CustomLoader)
```

## ğŸ“Š Monitoring and Logging

### Health Checks

- `GET /health` - Overall service health
- `GET /v1/tutor/health` - RAG-specific health
- `GET /v1/ai/statistics` - AI service statistics

### Logging

The service uses structured logging with loguru:

```python
# Configure custom logging
from loguru import logger

logger.add("ai_service.log", rotation="1 day", retention="30 days")
```

### Performance Metrics

- Request/response times
- API key usage and rotation
- Embedding cache hit rates
- Vector search performance
- Model success/failure rates

## ğŸ¤ Integration Examples

### Next.js Frontend Integration

```typescript
// Frontend integration example
const uploadDocument = async (file: File, subjectId: string) => {
  const formData = new FormData();
  formData.append('file', file);
  formData.append('subject_id', subjectId);
  
  const response = await fetch('/api/ai/v1/tutor/upload-material', {
    method: 'POST',
    body: formData,
  });
  
  return await response.json();
};

const askQuestion = async (question: string, subjectId: string) => {
  const response = await fetch('/api/ai/v1/tutor/ask-question', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ question, subject_id: subjectId }),
  });
  
  return await response.json();
};
```

### Proxy Configuration

```javascript
// next.config.js
module.exports = {
  async rewrites() {
    return [
      {
        source: '/api/ai/:path*',
        destination: 'http://localhost:8000/:path*',
      },
    ];
  },
};
```

## ğŸ” Troubleshooting

### Common Issues

1. **Service won't start**: Check GEMINI_KEYS configuration
2. **Upload fails**: Verify file size and type restrictions
3. **No context found**: Ensure documents are uploaded for the subject
4. **Slow responses**: Check embedding model loading and cache

### Debug Mode

Enable debug mode for detailed logging:

```bash
DEBUG=true
LOG_LEVEL=DEBUG
```

## ğŸ“ License

This project is part of the CK Quiz educational platform.

## ğŸ¤ Contributing

1. Follow the established architecture patterns
2. Add comprehensive tests for new features
3. Update documentation and type hints
4. Ensure all services implement proper interfaces

---

**Built with â¤ï¸ for education. Rin-chan is ready to help students learn! ğŸ“** 